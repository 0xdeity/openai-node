// File generated from our OpenAPI spec by Stainless.

import * as Core from '~/core';
import { APIResource } from '~/resource';

export class Answers extends APIResource {
  /**
   * Answers the specified question using the provided documents and examples.
   *
   * The endpoint first [searches](/docs/api-reference/searches) over provided
   * documents or files to find relevant context. The relevant context is combined
   * with the provided examples and question to create the prompt for
   * [completion](/docs/api-reference/completions).
   */
  create(
    body: AnswerCreateParams,
    options?: Core.RequestOptions,
  ): Promise<Core.APIResponse<AnswerCreateResponse>> {
    return this.post('/answers', { body, ...options });
  }
}

export interface AnswerCreateResponse {
  answers?: Array<string>;

  completion?: string;

  model?: string;

  object?: string;

  search_model?: string;

  selected_documents?: Array<AnswerCreateResponse.SelectedDocuments>;
}

export namespace AnswerCreateResponse {
  export interface SelectedDocuments {
    document?: number;

    text?: string;
  }
}

export interface AnswerCreateParams {
  /**
   * List of (question, answer) pairs that will help steer the model towards the tone
   * and answer format you'd like. We recommend adding 2 to 3 examples.
   */
  examples: Array<Array<string>>;

  /**
   * A text snippet containing the contextual information used to generate the
   * answers for the `examples` you provide.
   */
  examples_context: string;

  /**
   * ID of the model to use for completion. You can select one of `ada`, `babbage`,
   * `curie`, or `davinci`.
   */
  model: string;

  /**
   * Question to get answered.
   */
  question: string;

  /**
   * List of documents from which the answer for the input `question` should be
   * derived. If this is an empty list, the question will be answered based on the
   * question-answer examples.
   *
   * You should specify either `documents` or a `file`, but not both.
   */
  documents?: Array<string> | null;

  /**
   * If an object name is in the list, we provide the full information of the object;
   * otherwise, we only provide the object ID. Currently we support `completion` and
   * `file` objects for expansion.
   */
  expand?: Array<unknown> | null;

  /**
   * The ID of an uploaded file that contains documents to search over. See
   * [upload file](/docs/api-reference/files/upload) for how to upload a file of the
   * desired format and purpose.
   *
   * You should specify either `documents` or a `file`, but not both.
   */
  file?: string | null;

  /**
   * Modify the likelihood of specified tokens appearing in the completion.
   *
   * Accepts a json object that maps tokens (specified by their token ID in the GPT
   * tokenizer) to an associated bias value from -100 to 100. You can use this
   * [tokenizer tool](/tokenizer?view=bpe) (which works for both GPT-2 and GPT-3) to
   * convert text to token IDs. Mathematically, the bias is added to the logits
   * generated by the model prior to sampling. The exact effect will vary per model,
   * but values between -1 and 1 should decrease or increase likelihood of selection;
   * values like -100 or 100 should result in a ban or exclusive selection of the
   * relevant token.
   *
   * As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token
   * from being generated.
   */
  logit_bias?: unknown | null;

  /**
   * Include the log probabilities on the `logprobs` most likely tokens, as well the
   * chosen tokens. For example, if `logprobs` is 5, the API will return a list of
   * the 5 most likely tokens. The API will always return the `logprob` of the
   * sampled token, so there may be up to `logprobs+1` elements in the response.
   *
   * The maximum value for `logprobs` is 5.
   *
   * When `logprobs` is set, `completion` will be automatically added into `expand`
   * to get the logprobs.
   */
  logprobs?: number | null;

  /**
   * The maximum number of documents to be ranked by
   * [Search](/docs/api-reference/searches/create) when using `file`. Setting it to a
   * higher value leads to improved accuracy but with increased latency and cost.
   */
  max_rerank?: number | null;

  /**
   * The maximum number of tokens allowed for the generated answer
   */
  max_tokens?: number | null;

  /**
   * How many answers to generate for each question.
   */
  n?: number | null;

  /**
   * A special boolean flag for showing metadata. If set to `true`, each document
   * entry in the returned JSON will contain a "metadata" field.
   *
   * This flag only takes effect when `file` is set.
   */
  return_metadata?: boolean | null;

  /**
   * If set to `true`, the returned JSON will include a "prompt" field containing the
   * final prompt that was used to request a completion. This is mainly useful for
   * debugging purposes.
   */
  return_prompt?: boolean | null;

  /**
   * ID of the model to use for [Search](/docs/api-reference/searches/create). You
   * can select one of `ada`, `babbage`, `curie`, or `davinci`.
   */
  search_model?: string | null;

  /**
   * Up to 4 sequences where the API will stop generating further tokens. The
   * returned text will not contain the stop sequence.
   */
  stop?: string | Array<string> | null;

  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
   * make the output more random, while lower values like 0.2 will make it more
   * focused and deterministic.
   */
  temperature?: number | null;

  /**
   * A unique identifier representing your end-user, which can help OpenAI to monitor
   * and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
   */
  user?: string;
}
